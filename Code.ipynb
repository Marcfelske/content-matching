{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xy8uVQ6we6PH"
   },
   "source": [
    "# Script to calculate the relevance of twitter friends and follower networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R9GHzR4zgU89"
   },
   "source": [
    "# Install requirements via pip etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pOj6sjGVe6PL"
   },
   "source": [
    "Chose to run local or in colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DiXxu37mzo0T"
   },
   "outputs": [],
   "source": [
    "run_in = 'local'\n",
    "#run_in = 'colab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21676,
     "status": "ok",
     "timestamp": 1566993313360,
     "user": {
      "displayName": "DISPLAYNAME",
      "photoUrl": "PHOTOURL",
      "userId": "USERID"
     },
     "user_tz": -60
    },
    "id": "nireLKaYgZ-8",
    "outputId": "4bee0079-d7dc-470c-9d31-baa4d3c121b8"
   },
   "outputs": [],
   "source": [
    "if run_in == 'local':\n",
    "  import csv\n",
    "  import os\n",
    "  import pandas as pd\n",
    "  import numpy as np\n",
    "  import tweepy\n",
    "  import datetime\n",
    "  import time\n",
    "  import json\n",
    "  import sys\n",
    "  import traceback\n",
    "  import string\n",
    "  import nltk\n",
    "  from nltk.tokenize import word_tokenize\n",
    "  from nltk.corpus import stopwords\n",
    "  from nltk.stem.porter import PorterStemmer\n",
    "  import pprint\n",
    "  pp = pprint.PrettyPrinter(indent=4)\n",
    "  print('Local runtime has packages')\n",
    "  from fuzzywuzzy import fuzz\n",
    "  \n",
    "if run_in == 'colab':\n",
    "  print('Downloading packages has packages')\n",
    "  #!pip install pandas==0.23.4\n",
    "  !pip install tweepy\n",
    "  !pip install xlrd\n",
    "  !pip install XlsxWriter\n",
    "  import csv\n",
    "  import os, os.path\n",
    "  import pandas as pd\n",
    "  import numpy as np\n",
    "  import tweepy\n",
    "  import time\n",
    "  import json\n",
    "  import sys\n",
    "  import time\n",
    "  from datetime import datetime\n",
    "  import traceback\n",
    "  import string\n",
    "  import nltk\n",
    "  nltk.download(\"popular\")\n",
    "  from nltk.tokenize import word_tokenize\n",
    "  from nltk.corpus import stopwords\n",
    "  from nltk.stem.porter import PorterStemmer\n",
    "  import pprint\n",
    "  pp = pprint.PrettyPrinter(indent=4)\n",
    "  !pip install fuzzywuzzy\n",
    "  from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21671,
     "status": "ok",
     "timestamp": 1566993313366,
     "user": {
      "displayName": "DISPLAYNAME",
      "photoUrl": "PHOTOURL",
      "userId": "USERID"
     },
     "user_tz": -60
    },
    "id": "lAUQ2VGH-B6a",
    "outputId": "08981375-8c17-4f9c-b743-e8b1c4ec03e6"
   },
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R04uUCmrgpwm"
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WCbj94_hqO-R"
   },
   "outputs": [],
   "source": [
    "#Define what to analyse and whether to clean:\n",
    "#analyse = \"bi_directional\"\n",
    "analyse = \"whole network\"\n",
    "\n",
    "version_id = \"V14\"\n",
    "\n",
    "clean_existing = True # when true, the startups for which a file already exists will be dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvFQcCHGii7s"
   },
   "source": [
    "## Define your personal path to the folder we will work in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 41034,
     "status": "ok",
     "timestamp": 1566993332747,
     "user": {
      "displayName": "DISPLAYNAME",
      "photoUrl": "PHOTOURL",
      "userId": "USERID"
     },
     "user_tz": -60
    },
    "id": "8TgvDuAjg3vj",
    "outputId": "93f3cde5-9057-48fd-f9e8-b5564b5bab4d"
   },
   "outputs": [],
   "source": [
    "if run_in == 'local':\n",
    "  base_path = \"/users/USERNAME/Google Drive/USERNAME/\"\n",
    "if run_in == 'colab':\n",
    "  base_path = \"/content/drive/My Drive/USERNAME/\"\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  \n",
    "print('base_path set to:')\n",
    "print(base_path)\n",
    "\n",
    "os.listdir(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qaFK9l-Tsb31"
   },
   "source": [
    "## Import all company names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NlNaw3HxDJSd"
   },
   "outputs": [],
   "source": [
    "# already_analysed_companies=[]\n",
    "# if clean_existing:\n",
    "#     if analyse == 'bi_directional':\n",
    "#       network_relevance_output_path = base_path + \"Analysis/Network Relevance/Companies bi_directional/\"\n",
    "#       already_analysed_companies = [] # to be dropped\n",
    "\n",
    "#       for root, dirs, files in os.walk(network_relevance_output_path):\n",
    "#         for file in files:\n",
    "#             if file.endswith(\".json\"):\n",
    "#                 #print(os.path.splitext(file)[0])\n",
    "#                 already_analysed_companies.append(os.path.splitext(file)[0].replace(\"_bi_directional\", \"\"))\n",
    "    \n",
    "#     if analyse == 'whole network':\n",
    "#       network_relevance_output_path = base_path + \"Analysis/Network Relevance/Companies Whole Network/\"\n",
    "#       for root, dirs, files in os.walk(network_relevance_output_path):\n",
    "#         for file in files:\n",
    "#             if file.endswith(\".json\"):\n",
    "#                 #print(os.path.splitext(file)[0])\n",
    "#                 already_analysed_companies.append(os.path.splitext(file)[0])\n",
    "# print(already_analysed_companies)\n",
    "# print(len(already_analysed_companies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 701,
     "status": "error",
     "timestamp": 1566993378269,
     "user": {
      "displayName": "DISPLAYNAME",
      "photoUrl": "PHOTOURL",
      "userId": "USERID"
     },
     "user_tz": -60
    },
    "id": "etOGwSJKsfaJ",
    "outputId": "12a3d5aa-9477-4946-b1c8-9105f4cd7c14"
   },
   "outputs": [],
   "source": [
    "all_startup_file = os.path.join(\"/users/USERNAME/Google Drive/USERNAME/Pitchbook_Crunchbase_Tracxn_Raw/Don't touch V14/V14.xlsx\")\n",
    "#all_startup_file = os.path.join(base_path + \"Pitchbook_Crunchbase_Tracxn_Raw/Don't touch V14/V14.xlsx\")\n",
    "print(\"all_startup_file_path is: \" + all_startup_file + \" and the file exists: \" + str(os.path.isfile(all_startup_file)))\n",
    "company_name_df = pd.read_excel(all_startup_file, 'V14df_all', usecols = \"A\", skiprows=0).drop_duplicates()\n",
    "company_name_df.reset_index(drop=True, inplace=True)\n",
    "len_original = len(company_name_df)\n",
    "#company_name_df.head()\n",
    "\n",
    "print(\"We have found\", str(len_original), \"startups\")\n",
    "\n",
    "already_analysed_companies=[]\n",
    "if clean_existing:\n",
    "    if analyse == 'bi_directional':\n",
    "      network_relevance_output_path = base_path + \"Analysis/Network Relevance/Companies bi_directional/\"\n",
    "      already_analysed_companies = [] # to be dropped\n",
    "\n",
    "      for root, dirs, files in os.walk(network_relevance_output_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                #print(os.path.splitext(file)[0])\n",
    "                already_analysed_companies.append(os.path.splitext(file)[0].replace(\"_bi_directional\", \"\"))\n",
    "    \n",
    "    if analyse == 'whole network':\n",
    "      network_relevance_output_path = base_path + \"Analysis/Network Relevance/Companies Whole Network/\"\n",
    "      for root, dirs, files in os.walk(network_relevance_output_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                #print(os.path.splitext(file)[0])\n",
    "                already_analysed_companies.append(os.path.splitext(file)[0])\n",
    "                \n",
    "    print(already_analysed_companies)\n",
    "    print(len(already_analysed_companies))\n",
    "    \n",
    "    df_already_analysed = pd.DataFrame()\n",
    "    df_already_analysed[\"company_name\"] = already_analysed_companies\n",
    "    df_already_analysed[\"analysed\"] = \"analysed\"\n",
    "\n",
    "    company_name_df = company_name_df.merge(df_already_analysed, on='company_name', how='left')\n",
    "    company_name_df.fillna(\"not analysed\", inplace = True)\n",
    "    company_name_df = company_name_df.loc[company_name_df['analysed'] != \"analysed\"]\n",
    "    len_cleaned = len(company_name_df)\n",
    "    print(\"After cleaning already analysed companies (\", str(len_original-len_cleaned), \") we are left with\", str(len_cleaned), \"companies\")\n",
    "\n",
    "company_name_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GZlNsPyLDJSg"
   },
   "outputs": [],
   "source": [
    "#company_name_df = company_name_df.loc[company_name_df['company_name'] == \"9fin\"]\n",
    "#company_name_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pdtxO2nfDJSh"
   },
   "outputs": [],
   "source": [
    "#company_name_df = company_name_df[:100]\n",
    "#company_name_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MgtfrllU9JFL"
   },
   "source": [
    "## start_up database path\n",
    "This is the folder with .txt file for each start-up that contains all information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PX0uDH0_9C43"
   },
   "outputs": [],
   "source": [
    "start_up_db_path = os.path.join(base_path + \"Database/\")\n",
    "# os.listdir(start_up_db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z2Ukq1Ex7LDT"
   },
   "source": [
    "## user_ids database path\n",
    "This is the folder with .txt file for each user that contains all details and tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnT-m-de_yQM"
   },
   "outputs": [],
   "source": [
    "user_details_path = \"/Users/USERNAME/Desktop/Mining local/USERNAME/outputs/user_details/\"\n",
    "# os.listdir(user_details_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rAXF4MYRNupZ"
   },
   "outputs": [],
   "source": [
    "user_tweets_path = \"/Users/USERNAME/Desktop/Mining local/USERNAME/outputs/user_tweets/\"\n",
    "# os.listdir(user_tweets_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wExyx5r56mmn"
   },
   "source": [
    "## Def text_cleaner(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fh54A5Df6ltB"
   },
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "   # 1 Take all types of hyphens apart from U+002D \n",
    "   de_hyphened_em = str.replace(text, \"—\", \" \")\n",
    "   de_hyphened_en = str.replace(de_hyphened_em, \"–\", \" \")\n",
    "   # 2 Tokenize\n",
    "   tokens = word_tokenize(de_hyphened_en)\n",
    "   tokens = [item.replace(\"—\", \" \") for item in tokens]\n",
    "   # 3 Remove puctuation\n",
    "   table = str.maketrans('', '', string.punctuation)\n",
    "   stripped = [w.translate(table) for w in tokens]\n",
    "   # Remove lonely letters\n",
    "   no_single = [w for w in stripped if len(w) > 1]\n",
    "   # 4 Lower Case\n",
    "   low_words = [word.lower() for word in no_single]\n",
    "   # 5 Not Alpha\n",
    "   no_alpha = [w for w in low_words if w.isalpha()]\n",
    "   # 6 Delete Stop Words\n",
    "   stop_words = set(stopwords.words('english'))\n",
    "   no_stop = [w for w in no_alpha if not w in stop_words]\n",
    "   # 10 Stem Words\n",
    "   porter = PorterStemmer()\n",
    "   stemmed = [porter.stem(word) for word in no_stop] \n",
    "   \n",
    "   return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NIjz-mIkH35A"
   },
   "source": [
    "## Def get_data(start_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eiSZcM1SFzVS"
   },
   "outputs": [],
   "source": [
    "def get_data_company(start_up, clean = True):\n",
    "  \"\"\"Function which takes in a startup name in the form of a string (\"roborace\") returns the descriptions and website content of the startup\"\"\"\n",
    "  with open(start_up_db_path + start_up + '.json', 'r') as startup_json:\n",
    "        try:\n",
    "          data = json.load(startup_json)\n",
    "          company_name = data[0]\n",
    "          company_dict = data[1]\n",
    "          website_content = company_dict[\"website_content\"].replace(\"{\", \"\").replace(\"}\", \"\").replace(\"'\", \"\").replace(\",\", \"\").split()\n",
    "        except AttributeError: \n",
    "          print(\"There was no website for this company\")\n",
    "          website_content = []\n",
    "        \n",
    "        combined_desc = []\n",
    "        \n",
    "        try:\n",
    "          combined_desc.extend(data[1][\"desc_pitchbook\"].lower().split())\n",
    "          #print(1)\n",
    "          #print(combined_desc)\n",
    "        except: KeyError\n",
    "    \n",
    "        try:\n",
    "          combined_desc.extend(data[1][\"desc_crunchbase\"].lower().split())\n",
    "          #print(2)\n",
    "          #print(combined_desc)\n",
    "        except: KeyError\n",
    "        \n",
    "        try:\n",
    "          combined_desc.extend(data[1][\"desc_tracxn\"].lower().split())\n",
    "          #print(3)\n",
    "        except: KeyError\n",
    "            \n",
    "        company_match_content = website_content + combined_desc\n",
    "        company_match_content = set(company_match_content)\n",
    "  #print(company_name, company_dict, company_match_content)\n",
    "        if clean:\n",
    "            cmc = []\n",
    "            for word in company_match_content:\n",
    "              cmc += text_cleaner(word)\n",
    "        else:\n",
    "            cmc = company_match_content\n",
    "\n",
    "  return(company_name, company_dict, cmc)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hi7fZ2YjH-J4"
   },
   "source": [
    "## Def get_data(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3aTwL2rqH-c3"
   },
   "outputs": [],
   "source": [
    "def get_data_user(user_id, clean=True):\n",
    "  \"\"\"Function which takes in a user in the form of a string id (\"123382\") returns the tweets and description of the user\"\"\"\n",
    "  u = user_id\n",
    "  #print(u)\n",
    "  su = str(u)[:2] # takes the first two digits of the user id to find the folder\n",
    "  user_detail = user_details_path + su + \"/\" + str(u) + \".json\"\n",
    "  user_tweets = user_tweets_path + su + \"/\" + str(u) + \".txt\" # neu hinzugefügt\n",
    "\n",
    "  #print(user_detail)\n",
    "  #print(user_tweets)\n",
    "\n",
    "  if os.path.isfile(user_detail)==True:\n",
    "    #print(\"file does exist\")\n",
    "    with open(user_detail) as u_json:\n",
    "        u_data = json.load(u_json)\n",
    "        if u_data == 'error':\n",
    "          #print(\"user has no description or was not found\") \n",
    "          u_desc = []\n",
    "        else:  \n",
    "          #print(\"Step 1 u_data: \",u_data)\n",
    "          u_desc = u_data[\"description\"].replace(\"#\", \"\").split() # replace \"#\" so that they do not get cleaned by the text_cleaner\n",
    "          #print(type(u_desc))\n",
    "          #print(\"Step 2: \", u_desc)\n",
    "          #print(type(u_desc))\n",
    "  else:\n",
    "    u_desc = []\n",
    "    #print(\"file does not exist\")\n",
    "\n",
    "  #user_tweets = \"/content/drive/My Drive/Mark/outputs/user_tweets/84/844909155857129473.txt\"\n",
    "  if os.path.isfile(user_tweets)==True:\n",
    "    #print(\"file does exist\")\n",
    "    with open(user_tweets) as ut_txt:\n",
    "        ut_data = ut_txt.readlines()\n",
    "        ut_data = ut_data\n",
    "        u_tweet_content = []\n",
    "        #print(\"ut_data:\", ut_data)\n",
    "        for line in ut_data:\n",
    "          line = line[2:-1]\n",
    "          line = line.replace('RT ', '')\n",
    "          line = line.replace('\\\\n', '')\n",
    "          line = line.replace('#', '') # replace \"#\" so that they do not get cleaned by the text_cleaner\n",
    "          line = line.replace('\\\\', ' ')\n",
    "          line = line.replace('http', '')\n",
    "          u_tweet_content.append(line)\n",
    "          #print(type(u_tweet_content))\n",
    "    #u_tweet_content = ' '.join(u_tweet_content)\n",
    "    #print(\"tweet_content of\", u, u_tweet_content)\n",
    "    #print(type(u_tweet_content))\n",
    "  else:\n",
    "    u_tweet_content = []\n",
    "    #print(\"file does not exist\")\n",
    "    \n",
    "  user_match_content =  u_desc + u_tweet_content \n",
    "  #print(\"user_match_content:\", user_match_content)\n",
    "  \n",
    "  if clean:\n",
    "    umc = []\n",
    "    for word in user_match_content:\n",
    "      umc += text_cleaner(word)\n",
    "  else:\n",
    "    umc = user_match_contet\n",
    "  return(umc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "atriJ1SNwuX8"
   },
   "source": [
    "For every startup we want to count the most matched words and put them in a histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xyt10mpGbLri"
   },
   "source": [
    "## Def wordmatcher (lst1=[], lst2=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GQ3UpIsqbMOV"
   },
   "outputs": [],
   "source": [
    "def word_comp(w1, w2, tol=85):\n",
    "  \"\"\"compares two words and returns true if they are the same\"\"\"  \n",
    "  if int(fuzz.ratio(w1, w2)) > tol:\n",
    "    #print(w1, w2)\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "def wordmatcher(lst1=[], lst2=[]):\n",
    "  \"\"\"compares two lists and returns the matches\"\"\"  \n",
    "  #lst1 = [\"cheese\", \"apples\", \"goat\"]\n",
    "  #lst2 = [\"cheese\", \"cheese\", \"goat\", \"boat\"]\n",
    "  matches ={}\n",
    "  for w1 in lst1:\n",
    "    for w2 in lst2:\n",
    "      if word_comp(w1, w2):\n",
    "        if w1 not in matches:\n",
    "          matches[w1] = 0\n",
    "        matches[w1] += 1\n",
    "  return matches\n",
    "\n",
    "def wordmatcher_set(lst1=[], lst2=[]):\n",
    "  \"\"\"compares two lists and returns the matches\"\"\"  \n",
    "  unions = list(set(lst1).intersection(set(lst2)))\n",
    "  #lst1 = [\"cheese\", \"apples\", \"goat\"]\n",
    "  #lst2 = [\"cheese\", \"cheese\", \"goat\", \"boat\"]\n",
    "  matches ={}\n",
    "  for word in unions:\n",
    "    matches[word] = lst1.count(word)\n",
    "  return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pBrB3E04CAGt"
   },
   "source": [
    "# Match all startups with all their followers and friends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lld6X18WDJSs"
   },
   "source": [
    "## analyse == \"whole network\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jm5M2n4sDJSs"
   },
   "outputs": [],
   "source": [
    "print(\"This part of the script STARTED at:\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "utMCqRWSB-tE"
   },
   "outputs": [],
   "source": [
    "if analyse == \"whole network\":\n",
    "\n",
    "  versions = [\"V\"+str(i) for i in range(1,15)]\n",
    "  # open all paths to all startup.txt files which contain dictionaries  \n",
    "  overall_dict = {}\n",
    "  c_counter = 0\n",
    "  for index,row in company_name_df.iterrows():\n",
    "      c_counter += 1\n",
    "      start_up = row['company_name']\n",
    "      company_name, company_dict, company_match_content = get_data_company(start_up)\n",
    "      print(\"start_up: \" + start_up)\n",
    "      #print(\"company_dict: \" + str(company_dict))\n",
    "      #print(\"company_dict: \" + str(company_dict.keys()))\n",
    "      #print(\"company_match_content: \" + str(company_match_content))\n",
    "      #print(\"company_match_content: \" + str(company_match_content))\n",
    "      #overall_dict = {}\n",
    "      overall_dict[company_name] = {}\n",
    "      print(\"Company\", start_up,\":\", str(c_counter), \"/\", str(len(company_name_df)), str(datetime.datetime.now()))\n",
    "\n",
    "      # for a given startup iterate over the versions and obtain the friends_ids  \n",
    "      for version in versions:\n",
    "        print(version,\"/\",versions[-1])\n",
    "        overall_dict[company_name][version] = {}\n",
    "\n",
    "        if version in company_dict:\n",
    "          for ffid in [\"friends\", \"followers\"]:\n",
    "            print(ffid)\n",
    "            if ffid + \"_ids\" in company_dict[version]:\n",
    "                overall_dict[company_name][version][ffid] = {}\n",
    "                user_ids = company_dict[version][ffid+\"_ids\"]\n",
    "                user_ids = [str(u) for u in user_ids]\n",
    "                #print(\"user_ids:\",user_ids)\n",
    "\n",
    "                counter = 0\n",
    "                for u in user_ids:\n",
    "                  counter += 1\n",
    "                  if not counter % 50:\n",
    "                    print(\"For\", company_name, \"User:\", str(counter) + \"/\" + str(len(user_ids)))\n",
    "                  #print(\"u:\", u)\n",
    "                  #print(\"Now we analyse the individual users\")\n",
    "                  user_match_content = get_data_user(u)\n",
    "                  #print(company_name, version, ffid, u)\n",
    "                  #print(\"user_match_content is:\", user_match_content)\n",
    "                  #print(\"clean text:\", text_cleaner(user_match_content))\n",
    "                  # match words between stemmed desc+tweets AND startupdesc+website (match_content)\n",
    "                  #matched = wordmatcher_set(user_match_content, company_match_content)\n",
    "                  matched = wordmatcher_set(user_match_content, company_match_content)\n",
    "                  #print(\"matched:\",matched)\n",
    "                    \n",
    "                  sum_all_matches = 0\n",
    "                  num_matches = int(np.sum(list(matched.values()))) #number of matches per user\n",
    "                  sum_all_matches += num_matches #number of matches across users per company\n",
    "\n",
    "                  # populates the dictrionary per follower\n",
    "                  overall_dict[company_name][version][ffid][u] = {}\n",
    "                  overall_dict[company_name][version][ffid][u][\"word matches\"] = matched\n",
    "                  overall_dict[company_name][version][ffid][u][\"sum word matches\"] = num_matches\n",
    "\n",
    "                word_stats = {} # Used to save absolute number of matches per occuring word\n",
    "                #total_matched_words # Total number of matched words across all users\n",
    "                user_matches = {} # Total number of matched words per user\n",
    "                sum_all_users = 0 # the number of users per company that had matches\n",
    "                word_user_match = {} # Which users had descriptions or tweets that matched certain words\n",
    "                #print(\"Initialised: word_stats, user_matches, sum_all_users, word_user_match dictionaries\")\n",
    "\n",
    "                #loop over users constructing word_stats\n",
    "                for u in user_ids:\n",
    "                  m = overall_dict[company_name][version][ffid][u][\"word matches\"]\n",
    "                  for w in m:\n",
    "                    if w not in word_stats:\n",
    "                      word_stats[w] = 0\n",
    "                    word_stats[w] += m[w]\n",
    "                    if w not in word_user_match:\n",
    "                      word_user_match[w] = []\n",
    "                    word_user_match[w].append(u)\n",
    "                    user_matches[u] = overall_dict[company_name][version][ffid][u][\"sum word matches\"]\n",
    "                total_matched_words = 0\n",
    "                for w, n in word_stats.items():\n",
    "                   total_matched_words += n\n",
    "                for u,n in user_matches.items():\n",
    "                  if n > 0:\n",
    "                    sum_all_users += 1\n",
    "\n",
    "                overall_dict[company_name][version][\"startup\" + ffid + \"stats\"] = {}\n",
    "                overall_dict[company_name][version][\"startup\" + ffid + \"stats\"][\"word_stats\"] = word_stats\n",
    "                overall_dict[company_name][version][\"startup\" + ffid + \"stats\"][\"total_matched_words\"] = total_matched_words\n",
    "                overall_dict[company_name][version][\"startup\" + ffid + \"stats\"][\"user_matches\"] = user_matches\n",
    "                overall_dict[company_name][version][\"startup\" + ffid + \"stats\"][\"sum_all_users\"] = sum_all_users\n",
    "                overall_dict[company_name][version][\"startup\" + ffid + \"stats\"][\"word_user_matches\"] = word_user_match\n",
    "\n",
    "      #pp.pprint(overall_dict) \n",
    "      for key, value in overall_dict.items():\n",
    "        with open(base_path + 'Analysis/Network Relevance/Companies Whole Network/' + start_up + \".json\", \"w\") as su_net_rel_file:\n",
    "          json.dump((key, value), su_net_rel_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9WVEXj7YDJSw"
   },
   "outputs": [],
   "source": [
    "print(\"This part of the script STOPPED at:\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "amuJmbUjCDoD"
   },
   "source": [
    "# Match all startups with bi-directional followers and friends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFXIieh4DJSz"
   },
   "source": [
    "## analyse == \"bi_directional\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_jDwQ5WEDJSz"
   },
   "outputs": [],
   "source": [
    "print(\"This part of the script STARTED at:\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EicKDFUmDJS1"
   },
   "outputs": [],
   "source": [
    "if analyse == \"bi_directional\":\n",
    "  versions = [\"V\"+str(i) for i in range(1,15)]\n",
    "  #versions = [\"V14\"]\n",
    "  #print(\"versions:\",versions)\n",
    "  # open all paths to all startup.txt files which contain dictionaries  \n",
    "  c_counter = 0\n",
    "  overall_dict = {}\n",
    "  for index,row in company_name_df.iterrows():\n",
    "      c_counter += 1\n",
    "      start_up = row['company_name']\n",
    "      company_name, company_dict, company_match_content = get_data_company(start_up)\n",
    "      print(\"start_up: \" + start_up)\n",
    "      #print(\"company_dict: \" + str(company_dict))\n",
    "      #print(\"company_dict: \" + str(company_dict.keys()))\n",
    "      #print(\"got company_match_content\")\n",
    "      #print(\"company_match_content: \" + str(company_match_content))\n",
    "      #overall_dict = {}\n",
    "      overall_dict[company_name] = {}\n",
    "      print(\"Company\", start_up,\":\", str(c_counter), \"/\", str(len(company_name_df)), str(datetime.datetime.now()))\n",
    "\n",
    "      # for a given startup iterate over the versions and obtain the friends_ids  \n",
    "      for version in versions:\n",
    "        print(version,\"/\",versions[-1])\n",
    "        overall_dict[company_name][version] = {}\n",
    "\n",
    "        if version in company_dict:\n",
    "          bi_direct_nodes_version = []\n",
    "          with open(\"/users/USERNAME/Google Drive/USERNAME/Pitchbook_Crunchbase_Tracxn_Raw/Don't touch \" + version + \"/\" + version + \"_bi_direct_nodes.txt\") as node_file:\n",
    "            #print(\"Found users in company dict\")\n",
    "            for line in node_file:\n",
    "              #print(\"line:\", line)\n",
    "              bi_direct_nodes_version.append(line.strip())\n",
    "             \n",
    "          #######here i have to check which ones are bidirectional set(friends)intersection(set(followers))   \n",
    "          try:\n",
    "              friend_ids = company_dict[version][\"friends_ids\"]\n",
    "              follower_ids = company_dict[version][\"followers_ids\"]\n",
    "              fnf_ids = user_ids2 = list(set(friend_ids).intersection(set(follower_ids)))\n",
    "              user_ids = fnf_ids\n",
    "                    \n",
    "              overall_dict[company_name][version][\"fnf\"] = {}\n",
    "              counter = 0\n",
    "              for u in user_ids:\n",
    "                counter += 1\n",
    "                if not counter % 50:\n",
    "                  print(\"For\", company_name, \"User:\", str(counter) + \"/\" + str(len(user_ids)))\n",
    "                #print(\"u:\", u)\n",
    "                #print(\"Now we analyse the individual users\")\n",
    "                user_match_content = get_data_user(u)\n",
    "                #print(company_name, version, \"fnf\", u)\n",
    "                #print(\"user_match_content is:\", user_match_content)\n",
    "                #print(\"clean text:\", text_cleaner(user_match_content))\n",
    "                # match words between stemmed desc+tweets AND startupdesc+website (match_content)\n",
    "                matched = wordmatcher_set(user_match_content, company_match_content)\n",
    "                #matched = wordmatcher(user_match_content, company_match_content)\n",
    "                #print(\"matched:\",matched)  \n",
    "\n",
    "                sum_all_matches = 0\n",
    "                num_matches = int(np.sum(list(matched.values()))) #number of matches per user\n",
    "                sum_all_matches += num_matches #number of matches across users per company\n",
    "\n",
    "                # populates the dictrionary per follower\n",
    "                overall_dict[company_name][version][\"fnf\"][u] = {}\n",
    "                overall_dict[company_name][version][\"fnf\"][u][\"word matches\"] = matched\n",
    "                overall_dict[company_name][version][\"fnf\"][u][\"sum word matches\"] = num_matches\n",
    "\n",
    "              word_stats = {} # Used to save absolute number of matches per occuring word\n",
    "              #total_matched_words # Total number of matched words across all users\n",
    "              user_matches = {} # Total number of matched words per user\n",
    "              sum_all_users = 0 # the number of users per company that had matches\n",
    "              word_user_match = {} # Which users had descriptions or tweets that matched certain words\n",
    "              #print(\"Initialised: word_stats, user_matches, sum_all_users, word_user_match dictionaries\")\n",
    "\n",
    "              #loop over users constructing word_stats\n",
    "              for u in user_ids:\n",
    "                m = overall_dict[company_name][version][\"fnf\"][u][\"word matches\"]\n",
    "                for w in m:\n",
    "                  if w not in word_stats:\n",
    "                    word_stats[w] = 0\n",
    "                  word_stats[w] += m[w]\n",
    "                  if w not in word_user_match:\n",
    "                    word_user_match[w] = []\n",
    "                  word_user_match[w].append(u)\n",
    "                  user_matches[u] = overall_dict[company_name][version][\"fnf\"][u][\"sum word matches\"]\n",
    "              total_matched_words = 0\n",
    "              for w, n in word_stats.items():\n",
    "                total_matched_words += n\n",
    "              for u,n in user_matches.items():\n",
    "                if n > 0:\n",
    "                  sum_all_users += 1\n",
    "              #print(\"calculated word_stats, user_matches, sum_all_users, word_user_match ready to populate dictionaries\")\n",
    "\n",
    "              overall_dict[company_name][version][\"startup\" + \"fnf\" + \"stats\"] = {}\n",
    "              overall_dict[company_name][version][\"startup\" + \"fnf\" + \"stats\"][\"word_stats\"] = word_stats\n",
    "              overall_dict[company_name][version][\"startup\" + \"fnf\" + \"stats\"][\"total_matched_words\"] = total_matched_words\n",
    "              overall_dict[company_name][version][\"startup\" + \"fnf\" + \"stats\"][\"user_matches\"] = user_matches\n",
    "              overall_dict[company_name][version][\"startup\" + \"fnf\" + \"stats\"][\"sum_all_users\"] = sum_all_users\n",
    "              overall_dict[company_name][version][\"startup\" + \"fnf\" + \"stats\"][\"word_user_matches\"] = word_user_match\n",
    "              #print(\"populated word_stats, user_matches, sum_all_users, word_user_match dictionaries\")\n",
    "          except KeyError:\n",
    "              print(\"Either friend_ids or follower_ids couldn't be retrieved\")\n",
    "              \n",
    "      #pp.pprint(overall_dict) \n",
    "      for key, value in overall_dict.items():\n",
    "        with open(base_path + \"Analysis/Network Relevance/Companies bi_directional/\" + start_up + \"_bi_directional.json\", \"w\") as su_net_rel_file:\n",
    "          json.dump((key, value), su_net_rel_file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2019-08-23 Network relevance JSON whole network.ipynb",
   "provenance": [
    {
     "file_id": "19oyME_RLdYHsQ_AtBO76Y3_5FQh8MLgN",
     "timestamp": 1563809968461
    },
    {
     "file_id": "17l_UI0JZkmBpCNOTB4rqNK3xEFAzp5w2",
     "timestamp": 1559041046788
    },
    {
     "file_id": "1ZZAU4xDviZ3ctnsU3hoF-HCp9YE0avTT",
     "timestamp": 1552320999177
    },
    {
     "file_id": "10LIOf3jixGQtk4ZNNMac_e1XzFTDRZ3k",
     "timestamp": 1551990249644
    },
    {
     "file_id": "1tEm5KE2LU5vb7Z8CS_TmxvoNoXGJqmn8",
     "timestamp": 1551882476488
    },
    {
     "file_id": "1Hax9n8eN6LeTdSIFA_CGBvsftTCpulzV",
     "timestamp": 1550074157400
    },
    {
     "file_id": "132jyyBqwofbWt2jFJYYuvuopgID_NYhH",
     "timestamp": 1548758517783
    },
    {
     "file_id": "1MLEw7dVyeaiwQamvgSXHUJCgRMLB4CV_",
     "timestamp": 1547756794708
    },
    {
     "file_id": "180dYigfig4LFXGOpPynEAKdeYkxLeJDL",
     "timestamp": 1546685929686
    },
    {
     "file_id": "1_PNIY2pwaewXL_9Riw09Ac2-FHZxuBN5",
     "timestamp": 1543856224345
    },
    {
     "file_id": "1JOYBz1ZtMX6wRSWLvGMCZeuBG3x1PcZI",
     "timestamp": 1542791993465
    },
    {
     "file_id": "1ZeI0QPRQ0dnBZkWZF6bfBcqpoT1Bxeta",
     "timestamp": 1542402299249
    },
    {
     "file_id": "1aA44MVDybJuro43buQOoW6d2FAb31pDX",
     "timestamp": 1542072959850
    },
    {
     "file_id": "1CZHBsEB8iJnRgnzV2fqeCWpM8JsZSQRn",
     "timestamp": 1540312554450
    },
    {
     "file_id": "1lun_-O8fDymR3Pkdm1AsQ9FSAQh4OoLP",
     "timestamp": 1539342881084
    },
    {
     "file_id": "1jPb2-K0HlK9dOeHHd4ff7y6iXWjzRF09",
     "timestamp": 1539276885549
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
